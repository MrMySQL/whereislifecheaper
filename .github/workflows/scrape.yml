name: Daily Scrape

on:
  schedule:
    # Run at 3:00 AM UTC daily
    - cron: '0 3 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI
    inputs:
      scraper:
        description: 'Specific scraper to run (leave empty for all)'
        required: false
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright Chromium
        run: npx playwright install chromium --with-deps

      - name: Seed database
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: npm run seed

      - name: Sync exchange rates
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          NODE_ENV: production
        run: npm run rates:sync

      - name: Run scrapers
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          PLAYWRIGHT_HEADLESS: 'true'
          SCRAPER_MAX_RETRIES: '3'
          SCRAPER_TIMEOUT: '30000'
          SCRAPER_CONCURRENT_BROWSERS: '2'
          NODE_ENV: production
          GOOGLE_CLOUD_PROJECT: ${{ secrets.GOOGLE_CLOUD_PROJECT }}
          GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
        run: |
          if [ -n "${{ inputs.scraper }}" ]; then
            echo "Running specific scraper: ${{ inputs.scraper }}"
            npm run scraper:run -- ${{ inputs.scraper }}
          else
            echo "Running all scrapers"
            npm run scraper:run
          fi

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: logs/
          retention-days: 7
